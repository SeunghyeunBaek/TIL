# 1. 서버와 클라이언트 

*190318*

*파이썬으로 배우는 웹크롤러*

- Robots.txt : 크롤링 가능 유무를 명시한 파일

  - 데이터를 크롤링하면 데이터 소유권이 서버에서 웹사이트로 넘어온다. 크롤링 하기 전에 크롤링 가능 여부(데이터 이용 약관)를 확인해야 한다.

- 스크래핑(Scraping) : 크롤링한 데이터를 선별하는 작업, 크롤링과 동시에 전처리 작업을 진행.

- 클라이언트-서버 요청 방식(p18)
  - GET
    - 직접 URL을 입력해서 웹페이지를 요청하는 방식
      - ex) 링크 클릭
    - POST 방식이 아닌 요청 방식
    - 속도가 빠르기에 대부분 GET 방식을 사용

  - POST
    - 사용자로부터 받은 입력 데이터를 기반으로 웹페이지를 요청할 때
      - ex) ID, 비밀번호,회원가입, 게시판에 글쓰기
  - GET vs POST
  |             | GET                                              | POST   |
  | :---------- | :----------------------------------------------- | ------ |
  | 용량        | 255 byte                                         | 무제한 |
  | 보안        | 불리함 : 사용자 정보(ID 비밀번호)가 URL에 보인다 | O      |
  | 메모리/속도 | 빠름                                             | 느림   |

  - 응답 코드
    - 200 : 완전한 성공 cf) 20x : 성공은 했지만 뭔가 문제가 있음
    - 4xx : 클라이언트가 잘못된 요청한 경우
      - 404 : 요청한 주소(URL)가 없는 경우
      - 405 : 요청 방식이 잘못된 경우 ex) POST 만 받는 웹페이지에서 GET 방식으로 요청한 경우
    - 5xx : 서버 코드 자체가 잘못된 경우, 서버 코드를 수정하지 않는 이상 해결할 수 없음

# 2.크롤링

- request 모듈(p164)

  - 주소를 입력받아서 요청할 때

- 파싱 모듈(p192)

  - BS4 : 제일 많이 쓴다.
  - Selenium : BS4를 활용해 웹페이지 소스를 봤을 때 원하는 결과가 없다면 Selenium을 사용해야함

- request 모듈 실습

  - pip list 에서 설치된 모듈 확인
    - requests
    - beautifulsoup4
    - selenium

  ```python
  import requests
  url = 'https://www.google.com' #접속할 페이지 주소 확인
  response_get = requests.get(url) #Get 방식 : 응답코드 200
  response_post = requests.get(url) #Post 방식 : 응답코드 405 ; 잘못된 요청, google.com 은 post 방식의 요청을 받지 못한다.
  print(f'get:{response_get.status_code}') #200
  print(f'get:{response_post.status_code}') #405
  ```

- 헤더(p26)

  - 엔티티 헤더 - Content-type :  서버가 보내는 문서의 양식 전달
    - 데이터
      - 응답 결과로 전달되는 문서의 양식
      - Encoding Type
        - Encoding : 데이터를 다른 형식으로 바꾸는 작업(ex: 한글->16진수)
        - 한글 query는 인식이 안되기 때문에 16진수로 바꾸어 준다
        - 서버에서 Encoding된 query를 Decoding함
        - UTF8 은 전 세계 언어를 표현할 수 있다
    - 불특정 다수의 사이트를 크롤링 할때, 해당 사이트들의 문서 양식을 파악해야함

  ```python
  response.headers # header 정보는 Dictionary로 저장돼있음
  response.test # 응답결과를 문자열로 반환, 웹페이지 '소스보기' 결과와 같음
  ```

# 3. BS4(p202)

- xml 이나 html 문서를 분석해서 원하는 데이터를 쉽게 가져올 수 있도록 만든 모듈
- lxml : C로 만들어져서 빠르다. BS4는 lxml 로 구성됐다.

```python
#import module
#pip install beautifulsoup4
#pip install lxml
import bs4

#객체 생성 방법
html = ''
soup = bs4.BeautifulSoup(html,'lxml') # input = html, parser ; lxml 성능이 가장 좋다.

html = '<p>test</p>' # <p> 단락을 구분
soup = bs4.BeautifulSoup(html,'lxml') # html 양식을 붙여서 객체를 만든다. <html><body>...
print(soup)
print(soup.prettify()) # 들여쓰기, 세로쓰기 적용, 코드를 눈으로 확인해 보고 싶을 때 prettify() 함수를 쓴다.
```

- html 문서 구조(p35)
  - xml 중 하나
  - 웹브라우저가 화면을 구성하기 위해 필요한 데이터를 담고있다.
  - html 은 웹브라우저 화면을 구성하기 위한 설계도
  - Tag 는 데이터의 종류를 명시함
    - <나이>29</나이>
    - <주소>서울시..</주소>
    - *모든 html 에서 <title> 은 하나만 존재*
  - web = html + css + javascript 

```html
<!DOCTYPE html> <!--HTML5-->
<html lang = 'en'>
<head>
    <!--header : 웹브라우저에게 알려줘야할 정보-->
	<meta charset = 'UTF-8'>
	<title>Title</title>
</head>
<body>
<!--body : 웹페이지에서 보이는 부분-->
</body>
</html>
```
