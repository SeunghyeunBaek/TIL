# Spark 

-- master : 다중(yarn)/단일(yarn) 클러스터 지정 

-- class : 프로젝트 시작점, 패키지명, 클래스 명

-- name : 프로젝트 이름

~/spark : 패키지 파일 위치

- 실행 명령

spark-submit --master local

​			--class 패키지명, 클래스명, jar 경로 안의 패키지를 실행해라

​			--name 프로젝트명

​			~/spark target 안에 패키징된 Jar 파일 경로

- ex(프로젝트 이름만 변경하면 됨)

spark-submit --master local

​			--class com.test.spark.**HelloWorld**

​			--name **HelloWorld**

​			target.scala-2.10/**Hello World**-assembly-0.1.jar

- 입력

spark-submit --master local --class com.test.spark.HelloWorld --name HelloWorld target/scala-2.10/HelloWorld-assembly-0.1.jar

- target : sbt assembly를 정상실행하면 프로젝트 폴더안에 target 폴더가 생긴다. 이 안에 jar 패키지가 있다.



HelloWorld.scala

```scala
package com.test.spark

import org.apache.spark.{SparkConf, SparkContext} # 모듈 임포트

object HelloWorld{
	def main(args: Array[String]){
        
		val conf = new SparkConf
		val sc = new SparkContext(conf)
		
		try{
			println("========================")
			println("Hello Word")
			println("========================")			
		
		} finally{
			sc.stop()
		}
	}
}
```

- Object : 
  - cf) Class : 선언 이후 생성자를 통해 객체를 생성한다.
  - Object는 선언하는 동시에 객체를 생성한다.
    - 스칼라는 프로그램 실행 동시에 main 함수를 실행한다(Java와 비슷함)
    - Class로 바꾸고 실행하면 main을 호출할 수 없기 때문에 오류 발생
    - --class com.test.spark.Helloworld 명령어는 main 함수 위치를 설명함(프로그램 시작점)
  - Context
    - 작업을 하기 위해 필요한 정보를 담은 객체
      - ex) 그래픽 작업을 하기 위해 필요한 정보를 담은 객체 : 그래픽 Context
    - SparkConf 에는 Spark 운영을 위한 필요한 정보(프로젝트 정보, 하드웨어 정보 등등)
    - SparkContext(SparkConf)
- try - finally : 예외처리 , 파일이 없을 경우 stop()



## SundayCount.scala

*아파치 스파크 입문(p.79)*

파일로부터 날짜를 읽어와서 일요일의 개수를 세는 프로그램

build.sbt => libraryDependencies 에 사용할 라이브러리를 추가한다.

- RDD는 워커노드 개수만큼 분산된 리스트(list) 형태

```scala 
val textRDD  = sc.textFile(filePath) #  filePath에 있는 데이터를 읽어와서 RDD로 만든다.
```

build.sbt

```scala
name := "SundayCount"
version := "0.1"
scalaVersion := "2.10.4"
libraryDependencies ++= Seq(
	"org.apache.spark" % "spark-core_2.10" % "2.0.1" % "provided", // provided 컴퓨터 내부에 있는 라이브러리를 설치
    "joda-time" % "joda-time" % "2.8.2" // 라이브러리를 만든 그룹 % 라이브러리 이름 % 버전
)

assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)
```

src > main>scala>com>test>spark

```scala

```

spark-shell에서 코드를 먼저 테스트해본다.

명령어 : spark-shell







 